# -*- coding: utf-8 -*-
"""DLCV_Final_Competition_66.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QeWLho_gpTD8zHBOcZs81gGt3pdtLnew
"""

# ==================== Import Packages ====================
import os, random, shutil
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler
from torchvision import transforms
from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights
from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from google.colab import drive

# ==================== Mount Drive & Path ====================
drive.mount('/content/drive', force_remount=True)

base_drive_path = '/content/drive/MyDrive/Competition2/Competition/'
base_local_path = '/content/'

path_dataset     = os.path.join(base_local_path, 'dataset_beam/beam_damage/')
test_folder_path = os.path.join(base_local_path, 'dataset_beam/test/')
csv_file_path    = os.path.join(base_local_path, 'beam.csv')
csv_source_path  = os.path.join(base_drive_path, 'dataset_beam/beam.csv')

if not os.path.exists(path_dataset):
    shutil.copytree(os.path.join(base_drive_path, 'dataset_beam/beam_damage/'), path_dataset)
if not os.path.exists(test_folder_path):
    shutil.copytree(os.path.join(base_drive_path, 'dataset_beam/test'), test_folder_path)
if not os.path.exists(csv_file_path):
    shutil.copy(csv_source_path, csv_file_path)

# ==================== Settings ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(1220); np.random.seed(1220); random.seed(1220)

CLASS2IDX = {'A':0, 'B':1, 'C':2}
IDX2LABEL18 = {0:18, 1:19, 2:20}
CRIT_BY_CLS_BEAM = {
    0: [0],
    1: [3, 4, 6, 8],
    2: [1, 5, 7, 9, 10]
}
N_CRIT = 11

# ==================== Transform ====================
train_tf = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(0.1, 0.1, 0.1),
    transforms.RandomAffine(0, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

strong_train_tf = transforms.Compose([
    transforms.RandomApply([
        transforms.RandomRotation(degrees=30),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10)
    ], p=0.8),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])


test_tf = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# ✅ Oversample 少樣本 criteria 的 Dataset 包裝器
class OversampleCriteriaDataset(Dataset):
    def __init__(self, base_dataset, min_count=30):
        self.base_dataset = base_dataset
        self.expanded = []

        # 統計每個 criteria 的出現次數
        count = np.zeros(N_CRIT)
        for _, _, y_cri in base_dataset:
            for i, val in enumerate(y_cri):
                if val == 1:
                    count[i] += 1

        # 針對每個 index，如果包含稀有標籤，就多複製幾次
        for idx in range(len(base_dataset)):
            _, _, y_cri = base_dataset[idx]
            rare = any(y_cri[i] == 1 and count[i] < min_count for i in range(N_CRIT))
            repeat = 5 if rare else 1
            self.expanded.extend([idx] * repeat)

    def __len__(self):
        return len(self.expanded)

    def __getitem__(self, i):
        return self.base_dataset[self.expanded[i]]

# ==================== Dataset ====================
class BeamDamageDataset(Dataset):
    def __init__(self, root_dir, transform=None, strong_transform=None):
        self.paths, self.cls_labels, self.cri_labels = [], [], []
        self.transform = transform
        self.strong_transform = strong_transform
        self.cri_counts = np.zeros(N_CRIT)

        for cls_name in os.listdir(root_dir):
            if cls_name not in CLASS2IDX: continue
            cls_idx = CLASS2IDX[cls_name]
            valid_cr = sorted(CRIT_BY_CLS_BEAM[cls_idx])

            cls_folder = os.path.join(root_dir, cls_name)
            for img_name in os.listdir(cls_folder):
                if not img_name.lower().endswith('.jpg'): continue
                img_path = os.path.join(cls_folder, img_name)

                txt_path = os.path.splitext(img_path)[0] + '.txt'
                if os.path.exists(txt_path):
                    with open(txt_path, 'r') as f:
                        crit = {int(c.strip()) for c in f.read().split(',') if c.strip()}
                    crit &= set(valid_cr)
                else:
                    crit = set([valid_cr[0]])

                vec = []
                for i in range(N_CRIT):
                    if i in valid_cr:
                        vec.append(1.0 if i in crit else 0.0)
                        if i in crit:
                            self.cri_counts[i] += 1
                    else:
                        vec.append(-1.0)
                self.paths.append(img_path)
                self.cls_labels.append(cls_idx)
                self.cri_labels.append(vec)

        global CRIT_WEIGHT
        CRIT_WEIGHT = [3.0 / (c + 1e-6) if c > 0 else 0.0 for c in self.cri_counts]
        self.alpha = [2.0 / (c / max(self.cri_counts + 1e-6)) if c > 0 else 0.0 for c in self.cri_counts]

    def __len__(self): return len(self.paths)

    def __getitem__(self, idx):
        img = Image.open(self.paths[idx]).convert('RGB')
        if self.cls_labels[idx] == 0 and self.strong_transform:
            img = self.strong_transform(img)
        elif self.transform:
            img = self.transform(img)
        return (
            img,
            torch.tensor(self.cls_labels[idx], dtype=torch.long),
            torch.tensor(self.cri_labels[idx], dtype=torch.float32)
        )

# ==================== Masked Weighted Focal Loss ====================
class MaskedBCELoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, logits, targets):
        mask = (targets != -1).float()
        targets = torch.clamp(targets, min=0)
        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none') * mask
        return loss.sum() / (mask.sum() + 1e-6)

# ==================== MultiTask Model (ResNet50) ====================
class MultiTaskNet(nn.Module):
    def __init__(self):
        super().__init__()
        base = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)
        self.backbone = nn.Sequential(*list(base.features))
        self.pool = nn.AdaptiveAvgPool2d(1)
        in_ch = base.classifier[1].in_features

        self.cls_head = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(in_ch, 3)
        )
        self.cri_head = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(in_ch, N_CRIT)
        )

        for param in self.backbone.parameters():
            param.requires_grad = True

    def forward(self, x):
        x = self.pool(self.backbone(x))
        x = torch.flatten(x, 1)
        return self.cls_head(x), self.cri_head(x)


def mixup_data(x, y, alpha=0.4):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(pred, y_a, y_b, lam):
    return lam * F.cross_entropy(pred, y_a) + (1 - lam) * F.cross_entropy(pred, y_b)

# ==================== Dataloader ====================
def get_dataloaders(batch=16):
    ds_full = BeamDamageDataset(path_dataset, transform=train_tf, strong_transform=strong_train_tf)
    n_train = int(0.75 * len(ds_full))
    ds_train, ds_val = random_split(ds_full, [n_train, len(ds_full) - n_train])
    ds_val.dataset.transform = test_tf
    ds_val.dataset.strong_transform = None

    ds_train_os = OversampleCriteriaDataset(ds_train.dataset)
    tr_loader = DataLoader(ds_train_os, batch_size=batch, shuffle=True, num_workers=2)
    va_loader = DataLoader(ds_val, batch_size=batch, shuffle=False, num_workers=2)
    return tr_loader, va_loader

# ==================== Training ====================
def train_multitask(model, tr_loader, va_loader, epochs=50, lmbd=7.0):
    writer = SummaryWriter(log_dir='runs/beam_training')
    cls_weights = torch.tensor([1.0/120, 1.0/182, 1.0/150], device=device)
    cls_weights = cls_weights / cls_weights.min()
    loss_cls = nn.CrossEntropyLoss(weight=cls_weights)
    loss_cri = MaskedBCELoss()
    opt = optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-5)
    sch = CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-6)
    best_val = 1e9
    patience = 5
    wait = 0

    for ep in range(1, epochs + 1):
        model.train()
        tr_loss, cri_loss_sum, correct_cls, total = 0, 0, 0, 0
        for x, yc, yr in tqdm(tr_loader, desc=f'Epoch {ep}/{epochs}', ncols=95):
            x, yc, yr = x.to(device), yc.to(device), yr.to(device)
            opt.zero_grad()
            out_c, out_r = model(x)
            loss_c = loss_cls(out_c, yc)
            loss_r = loss_cri(out_r, yr)
            loss = loss_c + lmbd * loss_r
            loss.backward()
            opt.step()
            tr_loss += loss.item()
            cri_loss_sum += loss_r.item()
            correct_cls += (torch.argmax(out_c, 1) == yc).sum().item()
            total += yc.size(0)

        model.eval()
        val_loss, val_cls_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for x, yc, yr in va_loader:
                x, yc, yr = x.to(device), yc.to(device), yr.to(device)
                out_c, out_r = model(x)
                val_loss += (loss_cls(out_c, yc) + lmbd * loss_cri(out_r, yr)).item()
                val_cls_correct += (torch.argmax(out_c, 1) == yc).sum().item()
                val_total += yc.size(0)

        tr_loss /= len(tr_loader)
        val_loss /= len(va_loader)
        acc_tr = correct_cls / total
        acc_va = val_cls_correct / val_total
        print(f'  ▶ train={tr_loss:.4f} | val={val_loss:.4f} | acc_tr={acc_tr:.3f} | acc_val={acc_va:.3f} | cri_loss={cri_loss_sum/len(tr_loader):.4f}')

        writer.add_scalar('Loss/train', tr_loss, ep)
        writer.add_scalar('Loss/val', val_loss, ep)
        writer.add_scalar('Loss/cri_train', cri_loss_sum / len(tr_loader), ep)
        writer.add_scalar('Acc/train', acc_tr, ep)
        writer.add_scalar('Acc/val', acc_va, ep)

        if val_loss < best_val:
            best_val = val_loss
            wait = 0
            torch.save(model.state_dict(), 'best_multitask_model.pth')
        else:
            wait += 1
            if wait >= patience:
                print("Early stopping triggered.")
                break
        sch.step()
    writer.close()
    print('✓ Training finished. Best val loss:', best_val)


# ==================== Evaluation: F1-score & Confusion Matrix ====================
def evaluate_model(model, val_loader):
    model.eval()
    y_true_cls, y_pred_cls, y_true_cri, y_pred_cri = [], [], [], []
    with torch.no_grad():
        for x, yc, yr in val_loader:
            x, yc, yr = x.to(device), yc.to(device), yr.to(device)
            out_c, out_r = model(x)
            y_pred_cls.extend(torch.argmax(out_c, 1).cpu().numpy())
            y_true_cls.extend(yc.cpu().numpy())
            y_pred_cri.extend((torch.sigmoid(out_r) > 0.55).float().cpu().numpy())
            y_true_cri.extend((yr > 0).float().cpu().numpy())

    print("\nClassification Report (Classes):")
    print(classification_report(y_true_cls, y_pred_cls, target_names=['A', 'B', 'C'], zero_division=0))

    print("\nF1 Score per Critical Attribute:")
    f1_scores = f1_score(y_true_cri, y_pred_cri, average=None, zero_division=0)
    for i, f1 in enumerate(f1_scores):
        if i in [0, 1, 3, 4, 5, 6, 7, 8, 9, 10]:
            print(f"Attribute {i}: F1 = {f1:.3f}")
    print("Macro F1 (Critical Attributes):", f1_score(y_true_cri, y_pred_cri, average='macro', zero_division=0))

    for cls in range(3):
        valid_indices = [i for i, true in enumerate(y_true_cls) if true == cls]
        avg_attrs = np.mean([sum(y_pred_cri[i][CRIT_BY_CLS_BEAM[cls]]) for i in valid_indices])
        print(f"Average predicted attributes for Class {['A', 'B', 'C'][cls]}: {avg_attrs:.2f}")

    cm = confusion_matrix(y_true_cls, y_pred_cls)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['A', 'B', 'C'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Validation Confusion Matrix")
    plt.show()


# ✅ 自動尋找每個 criteria 的最佳 threshold（根據 validation set）
def find_optimal_thresholds(model, val_loader):
    model.eval()
    y_true, y_pred_logits = [], []
    with torch.no_grad():
        for x, _, yr in val_loader:
            x, yr = x.to(device), yr.to(device)
            _, out_r = model(x)
            y_true.extend((yr > 0).float().cpu().numpy())
            y_pred_logits.extend(torch.sigmoid(out_r).cpu().numpy())

    y_true = np.array(y_true)
    y_pred_logits = np.array(y_pred_logits)
    best_thresholds = []
    for i in range(N_CRIT):
        best_f1, best_thr = 0, 0.5
        for t in np.linspace(0.05, 0.95, 19):
            f1 = f1_score(y_true[:, i], (y_pred_logits[:, i] > t).astype(int), zero_division=0)
            if f1 > best_f1:
                best_f1, best_thr = f1, t
        print(f"Criteria {i}: best threshold = {best_thr:.2f}, F1 = {best_f1:.3f}")
        best_thresholds.append(best_thr)
    return best_thresholds

# ==================== Inference (TTA) ====================
def predict_and_save(model, csv_in, img_folder, csv_out, thr_list=None):
    model.eval()
    df = pd.read_csv(csv_in)
    outs = []

    def tta(img):
        augs = [test_tf(img), test_tf(transforms.functional.hflip(img))]
        return torch.stack(augs)

    with torch.no_grad():
        for _, row in df.iterrows():
            img_id = str(int(row['ID'])).zfill(0)
            img_path = os.path.join(img_folder, f'{img_id}.jpg')
            if not os.path.exists(img_path):
                outs.append({'ID': img_id, 'output': '-1'})
                continue

            img = Image.open(img_path).convert('RGB')
            inps = tta(img).to(device)
            log_c, log_r = model(inps)
            log_c, log_r = log_c.mean(0), log_r.mean(0)

            cls_idx = torch.argmax(log_c).item()
            cls_label = IDX2LABEL18[cls_idx]

            probs = torch.sigmoid(log_r).cpu().numpy()
            validID = CRIT_BY_CLS_BEAM[cls_idx]
            critSel = []

            for i in validID:
                thr = thr_list[i] if thr_list else 0.55
                if probs[i] > thr:
                    critSel.append((i, probs[i]))

            if len(critSel) == 0:
                best_i = validID[np.argmax([probs[i] for i in validID])]
                critSel = [(best_i, probs[best_i])]

            critSel = sorted(critSel, key=lambda x: x[1], reverse=True)[:4]
            crit_str = [str(i) for i, _ in critSel]
            outs.append({'ID': img_id, 'class': ','.join([str(cls_label)] + crit_str)})

    pd.DataFrame(outs).to_csv(csv_out, index=False)
    print(f'✓ Submission saved to {csv_out}')

# ==================== Run ====================
if __name__ == '__main__':
    tr_loader, va_loader = get_dataloaders(batch=16)
    net = MultiTaskNet().to(device)
    train_multitask(net, tr_loader, va_loader, epochs=50, lmbd=5.0)

    net.load_state_dict(torch.load('best_multitask_model.pth', map_location=device))
    evaluate_model(net, va_loader)

    best_thresholds = find_optimal_thresholds(net, va_loader)
    predict_and_save(net, csv_file_path, test_folder_path, '/content/beam_submission.csv', thr_list=best_thresholds)